\documentclass{article}
\usepackage{amsmath, amssymb, graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\lstdefinelanguage{JavaScript}{
  keywords={break, case, catch, continue, debugger, default, delete, do, else, finally, for, function, if, in, instanceof, new, return, switch, throw, try, typeof, var, void, while, with, let, const},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{codegreen}\ttfamily,
  stringstyle=\color{codepurple}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{CS2241 Assignment 1}
\author{Nico Fidalgo}
\date{March 26, 2025}

\begin{document}

\maketitle

\section{Problem 1: PageRank and HITS Algorithm Analysis}

\subsection{Problem Statement}
We are given a directed graph represented by the following adjacency matrix.

\begin{equation}
A = 
\begin{pmatrix}
0 & 0 & 1 & 1 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 0 & 1 \\
0 & 1 & 1 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0
\end{pmatrix}
\end{equation}

We need to calculate the PageRank scores and Hub and Authority scores. The interpretation of the adjacency matrix is:

\begin{itemize}
    \item Node $a$ has outgoing edges to nodes $c$, $d$, and $f$
    \item Node $b$ has outgoing edges to nodes $c$ and $f$
    \item Node $c$ has outgoing edges to nodes $d$ and $f$
    \item Node $d$ has outgoing edges to nodes $b$ and $c$
    \item Node $e$ has an outgoing edge to node $a$
    \item Node $f$ has an outgoing edge to node $e$
\end{itemize}

\subsection{PageRank Algorithm}
PageRank was designed to model the behavior of a random surfer on the web. The algorithm assigns a score to each node in a directed graph based on its structural importance. The intuition is that a node is important if many important nodes point to it and the importance of a node is divided among its outgoing links.

\begin{equation}
\mathbf{p} = (1-d) \cdot \frac{\mathbf{1}}{n} + d \cdot M \cdot \mathbf{p}
\end{equation}

where:
\begin{itemize}
    \item $d$ is the damping factor (typically 0.85)
    \item $n$ is the number of nodes
    \item $\mathbf{1}$ is a vector of all 1's
    \item $M$ is the transition matrix (columns sum to 1)
\end{itemize}

For each node $i$ with outgoing links, we set:
\begin{equation}
M_{j,i} = \frac{1}{\text{out-degree}(i)}
\end{equation}
if there is a link from node $i$ to node $j$, and 0 otherwise. The algorithm for computing PageRank scores is:

\begin{algorithm}
\caption{PageRank Score Calculation}
\begin{algorithmic}[1]
\State Initialize $\mathbf{p}^{(0)} = \frac{1}{n} \cdot \mathbf{1}$
\For{$t = 0, 1, 2, \ldots$ until convergence}
    \State $\mathbf{p}^{(t+1)} = (1-d) \cdot \frac{\mathbf{1}}{n} + d \cdot M \cdot \mathbf{p}^{(t)}$
    \If{$\|\mathbf{p}^{(t+1)} - \mathbf{p}^{(t)}\| < \text{tolerance}$}
        \State \textbf{break}
    \EndIf
\EndFor
\State Normalize $\mathbf{p}$ to sum to 1
\end{algorithmic}
\end{algorithm}

\subsection{PageRank Implementation}
I wrote a JavaScript program to implement the PageRank algorithm:

\begin{lstlisting}[language=JavaScript]
import * as math from 'mathjs';

// Adjacency matrix from the problem
const A = [
    [0, 0, 1, 1, 0, 1],  // a -> *
    [0, 0, 1, 0, 0, 1],  // b -> *
    [0, 0, 0, 1, 0, 1],  // c -> *
    [0, 1, 1, 0, 0, 0],  // d -> *
    [1, 0, 0, 0, 0, 0],  // e -> *
    [0, 0, 0, 0, 1, 0]   // f -> *
];

// Number of nodes
const n = A.length;

// Out-degrees
const out_degrees = A.map(row => row.reduce((sum, val) => sum + val, 0));
console.log("Out-degrees:", out_degrees);

// Create transition matrix (column-stochastic)
const M = Array(n).fill().map(() => Array(n).fill(0));
for (let i = 0; i < n; i++) {
    for (let j = 0; j < n; j++) {
        if (A[i][j] > 0) {
            M[j][i] = 1.0 / out_degrees[i];
        }
    }
}

// PageRank parameters
const d = 0.85;  // Damping factor
const max_iter = 100;
const tol = 1e-6;

// Initialize PageRank
let pr = Array(n).fill(1/n);

// Algorithm iteration
for (let iter = 0; iter < max_iter; iter++) {
    // Calculate M * pr
    const M_pr = Array(n).fill(0);
    for (let i = 0; i < n; i++) {
        for (let j = 0; j < n; j++) {
            M_pr[i] += M[i][j] * pr[j];
        }
    }
    
    // Calculate (1-d)/n + d * (M * pr)
    const pr_new = M_pr.map(val => (1-d)/n + d * val);
    
    // Check convergence
    const diff = math.norm(pr_new.map((val, idx) => val - pr[idx]));
    if (diff < tol) {
        pr = pr_new;
        console.log(`\nPageRank converged after ${iter+1} iterations.`);
        break;
    }
    
    pr = pr_new;
}

// Normalize to sum to 1
const pr_sum = pr.reduce((sum, val) => sum + val, 0);
pr = pr.map(val => val / pr_sum);
\end{lstlisting}

\subsection{PageRank Results}
After running the algorithm, I obtained the following PageRank scores:

\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Node} & \textbf{PageRank Score} \\
\hline
$a$ & 0.186551 \\
$b$ & 0.091079 \\
$c$ & 0.182643 \\
$d$ & 0.155479 \\
$e$ & 0.190060 \\
$f$ & 0.194188 \\
\hline
\end{tabular}
\end{center}

Nodes $f$, $e$, and $a$ have the highest PageRank scores, indicating they are structurally important in this network. Node $b$ has the lowest score, suggesting it's less central in the graph's link structure.

\subsection{HITS Algorithm}
The Hyperlink-Induced Topic Search (HITS) algorithm identifies two types of important nodes in a directed graph: hubs which are nodes that point to many good authorities, and authorities which are nodes that are pointed to by many good hubs. The hub ($\mathbf{h}$) and authority ($\mathbf{a}$) vectors satisfy:
\begin{align}
\mathbf{a} &= A^T \mathbf{h} \\
\mathbf{h} &= A \mathbf{a}
\end{align}

where $A$ is the adjacency matrix of the graph. The algorithm for computing HITS scores is:

\begin{algorithm}
\caption{HITS Score Calculation}
\begin{algorithmic}[1]
\State Initialize $\mathbf{h}^{(0)} = \mathbf{1}$ and $\mathbf{a}^{(0)} = \mathbf{1}$
\For{$t = 0, 1, 2, \ldots$ until convergence}
    \State $\mathbf{a}^{(t+1)} = A^T \mathbf{h}^{(t)}$
    \State Normalize $\mathbf{a}^{(t+1)}$
    \State $\mathbf{h}^{(t+1)} = A \mathbf{a}^{(t+1)}$
    \State Normalize $\mathbf{h}^{(t+1)}$
    \If{$\|\mathbf{a}^{(t+1)} - \mathbf{a}^{(t)}\| < \text{tolerance}$ and $\|\mathbf{h}^{(t+1)} - \mathbf{h}^{(t)}\| < \text{tolerance}$}
        \State \textbf{break}
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{HITS Implementation}
Again, I wrote a JavaScript program to implement the HITS algorithm:

\begin{lstlisting}[language=JavaScript]
import * as math from 'mathjs';

// Adjacency matrix from the problem
const A = [
    [0, 0, 1, 1, 0, 1],  // a -> *
    [0, 0, 1, 0, 0, 1],  // b -> *
    [0, 0, 0, 1, 0, 1],  // c -> *
    [0, 1, 1, 0, 0, 0],  // d -> *
    [1, 0, 0, 0, 0, 0],  // e -> *
    [0, 0, 0, 0, 1, 0]   // f -> *
];

// Number of nodes
const n = A.length;

// HITS parameters
const max_iter = 100;
const tol = 1e-6;

// Initialize hub and authority scores
let hub = Array(n).fill(1);
let auth = Array(n).fill(1);

// Compute transpose of A
const AT = Array(n).fill().map(() => Array(n).fill(0));
for (let i = 0; i < n; i++) {
    for (let j = 0; j < n; j++) {
        AT[i][j] = A[j][i];
    }
}

// HITS iteration
for (let iter = 0; iter < max_iter; iter++) {
    // Update authority scores: a = A^T * h
    const auth_new = Array(n).fill(0);
    for (let i = 0; i < n; i++) {
        for (let j = 0; j < n; j++) {
            auth_new[i] += AT[i][j] * hub[j];
        }
    }
    
    // Normalize authority scores
    const auth_norm = math.norm(auth_new);
    const auth_normalized = auth_new.map(val => val / auth_norm);
    
    // Update hub scores: h = A * a
    const hub_new = Array(n).fill(0);
    for (let i = 0; i < n; i++) {
        for (let j = 0; j < n; j++) {
            hub_new[i] += A[i][j] * auth_normalized[j];
        }
    }
    
    // Normalize hub scores
    const hub_norm = math.norm(hub_new);
    const hub_normalized = hub_new.map(val => val / hub_norm);
    
    // Check convergence
    const auth_diff = math.norm(auth_normalized.map((val, idx) => val - auth[idx]));
    const hub_diff = math.norm(hub_normalized.map((val, idx) => val - hub[idx]));
    
    if (auth_diff < tol && hub_diff < tol) {
        auth = auth_normalized;
        hub = hub_normalized;
        console.log(`HITS converged after ${iter+1} iterations.`);
        break;
    }
    
    auth = auth_normalized;
    hub = hub_normalized;
}
\end{lstlisting}

\subsection{HITS Results}
After running the HITS algorithm, I obtained the following scores:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Node} & \textbf{Hub Score} & \textbf{Authority Score} \\
\hline
$a$ & 0.684439 & 0.000000 \\
$b$ & 0.501536 & 0.113935 \\
$c$ & 0.446890 & 0.590796 \\
$d$ & 0.283360 & 0.454889 \\
$e$ & 0.000000 & 0.000000 \\
$f$ & 0.000000 & 0.656548 \\
\hline
\end{tabular}
\end{center}

Nodes $a$, $b$, and $c$ have high hub scores, indicating they are good at pointing to authority nodes. This makes sense as $a$ and $b$ point to multiple nodes including high authority nodes $c$ and $f$. Nodes $e$ and $f$ have zero hub scores because they don't point to any nodes with high authority scores.

Nodes $f$ and $c$ have the highest authority scores, followed by node $d$. This means they are pointed to by good hub nodes. Again, this makes sense as node $f$ and $c$ are pointed to by nodes $a$ and $b$ which have high hub scores. Nodes $a$ and $e$ have zero authority scores because they aren't pointed to by any good hub nodes.

\subsection{Assumptions}
For PageRank scores:
\begin{itemize}
    \item Used a damping factor of 0.85 (standard value)
    \item Defined convergence as when L2 norm difference $< 10^{-6}$
\end{itemize}

For HITS:
\begin{itemize}
    \item Defined convergence as when L2 norm difference $< 10^{-6}$
\end{itemize}

\section{Problem 2: Random Walk Methods for Hub and Authority Scores}

\subsection{Problem Statement}
We need to prove that the Hub score for a page is proportional to the number of outlinks and that the Authority score is proportional to the number of inlinks when using the following random walk approach:


\textbf{For Authority scores:}
\begin{itemize}
    \item From page $p_1$, follow back a random inlink to page $p_2$
    \item From $p_2$, follow forward a random outlink to page $p_3$
    \item The step takes us from $p_1$ to $p_3$
\end{itemize}

\textbf{For Hub scores:}
\begin{itemize}
    \item From page $p_1$, follow forward a random outlink to page $p_2$
    \item From $p_2$, follow backward a random inlink to page $p_3$
    \item The step takes us from $p_1$ to $p_3$
\end{itemize}

We assume the Markov chains are finite, irreducible, and aperiodic, ensuring a unique stationary distribution.

\subsection{Definitions}
\begin{itemize}
    \item $A[i,j] = 1$ if there's a link from page $i$ to page $j$, 0 otherwise
    \item $\text{in}(j) = \text{number of inlinks to page } j = \sum_i A[i,j]$
    \item $\text{out}(i) = \text{number of outlinks from page } i = \sum_j A[i,j]$
\end{itemize}

\subsection{Transition Probabilities}

\textbf{For Authority Random Walk:}\\
When starting at page $i$:
\begin{itemize}
    \item The probability of following a random inlink back to page $k$ is $\frac{A[k,i]}{\text{in}(i)}$
    \item The probability of following a random outlink from $k$ to $j$ is $\frac{A[k,j]}{\text{out}(k)}$
\end{itemize}

Therefore, the transition probability from $i$ to $j$ is:
\begin{equation}
    P_a(i,j) = \sum_k \frac{A[k,i]}{\text{in}(i)} \times \frac{A[k,j]}{\text{out}(k)}
\end{equation}

\textbf{For Hub Random Walk:}\\
When starting at page $i$:
\begin{itemize}
    \item The probability of following a random outlink to page $k$ is $\frac{A[i,k]}{\text{out}(i)}$
    \item The probability of following a random inlink back from $k$ to $j$ is $\frac{A[j,k]}{\text{in}(k)}$
\end{itemize}

Therefore, the transition probability from $i$ to $j$ is:
\begin{equation}
    P_h(i,j) = \sum_k \frac{A[i,k]}{\text{out}(i)} \times \frac{A[j,k]}{\text{in}(k)}
\end{equation}

\subsection{Authority Score Proof}

Let's hypothesize that the stationary distribution $\pi_a(i)$ is proportional to $\text{in}(i)$, i.e., $\pi_a(i) = c \times \text{in}(i)$ for some constant $c$.

For this to be a stationary distribution, it must satisfy:
\begin{equation}
    \pi_a(j) = \sum_i \pi_a(i) P_a(i,j)
\end{equation}

Substituting our hypothesis:
\begin{align}
    \pi_a(j) &= \sum_i c \times \text{in}(i) \times \sum_k \frac{A[k,i]}{\text{in}(i)} \times \frac{A[k,j]}{\text{out}(k)}\\
    &= c \times \sum_i \sum_k \frac{A[k,i] \times A[k,j]}{\text{out}(k)}\\
    &= c \times \sum_k \frac{A[k,j]}{\text{out}(k)} \times \sum_i A[k,i]
\end{align}

Since $\sum_i A[k,i] = \text{out}(k)$ (the number of outlinks from page $k$):
\begin{align}
    \pi_a(j) &= c \times \sum_k \frac{A[k,j]}{\text{out}(k)} \times \text{out}(k)\\
    &= c \times \sum_k A[k,j]\\
    &= c \times \text{in}(j)
\end{align}

This confirms the hypothesis that the Authority score $\pi_a(j)$ is proportional to $\text{in}(j)$, the number of inlinks to page $j$.

\subsection{Hub Score Proof}

Similarly, hypothesize that the stationary distribution $\pi_h(i)$ is proportional to $\text{out}(i)$, i.e., $\pi_h(i) = d \times \text{out}(i)$ for some constant $d$.

For this to be a stationary distribution, it must satisfy:
\begin{equation}
    \pi_h(j) = \sum_i \pi_h(i) P_h(i,j)
\end{equation}

Substituting our hypothesis:
\begin{align}
    \pi_h(j) &= \sum_i d \times \text{out}(i) \times \sum_k \frac{A[i,k]}{\text{out}(i)} \times \frac{A[j,k]}{\text{in}(k)}\\
    &= d \times \sum_i \sum_k \frac{A[i,k] \times A[j,k]}{\text{in}(k)}\\
    &= d \times \sum_k \frac{A[j,k]}{\text{in}(k)} \times \sum_i A[i,k]
\end{align}

Since $\sum_i A[i,k] = \text{in}(k)$ (the number of inlinks to page $k$):
\begin{align}
    \pi_h(j) &= d \times \sum_k \frac{A[j,k]}{\text{in}(k)} \times \text{in}(k)\\
    &= d \times \sum_k A[j,k]\\
    &= d \times \text{out}(j)
\end{align}

This confirms the hypothesis that the Hub score $\pi_h(j)$ is proportional to $\text{out}(j)$, the number of outlinks from page $j$.

\section{Problem 3: Reservoir Sampling}

\subsection{Single-Item Reservoir Sampling}
We need to prove that the following algorithm maintains a uniform sample of one item among all items seen so far:
\begin{itemize}
    \item When the first item appears, store it in memory
    \item When the $k$-th item appears, replace the current item with probability $1/k$
\end{itemize}

I will prove by induction that after processing $k$ items, each item has exactly $1/k$ probability of being in memory. In the base case where $k=1$, after seeing the 1st item, it is stored with probability 1. Since we've only seen one item, this gives us a uniform distribution. Beyond the base case, assume that after seeing $k-1$ items, each item has a probability of $1/(k-1)$ of being in memory. Now the $k$-th item arrives. For the $k$-th item, the probability it replaces the current item is $1/k$, therefore, $P(k\text{-th item in memory}) = 1/k$. For any previous item $i$ (where $1 \leq i < k$):

\begin{align}
    P(i\text{-th item in memory after $k$ items}) &= P(i\text{-th item was in memory}) \times P(\text{it remains in memory}) \\
    &= \frac{1}{k-1} \times \left(1 - \frac{1}{k}\right) \\
    &= \frac{1}{k-1} \times \frac{k-1}{k} \\
    &= \frac{1}{k}
\end{align}

Thus, after processing the $k$-th item, each of the $k$ items seen so far has exactly $1/k$ probability of being in memory, which confirms the uniform distribution property.

\subsection{Sampling $s$ Items Without Replacement}

Now we generalize to maintaining a uniform sample of $s$ items without replacement. The algorithm is:
\begin{enumerate}
    \item Store the first $s$ items as they arrive
    \item When the $k$-th item arrives (where $k > s$):
    \begin{itemize}
        \item With probability $s/k$, include it in the sample
        \item If including it, replace a randomly chosen item from the current sample
    \end{itemize}
\end{enumerate}

I need to prove that after seeing $k$ items ($k \geq s$), each item has exactly $s/k$ probability of being in the sample. In the base case, after seeing $s$ items, all $s$ items are in the sample with probability 1. Since $s/s = 1$, this is uniform. Beyond the base case, assume that after seeing $k-1$ items ($k-1 \geq s$), each has probability $s/(k-1)$ of being in the sample. Now the $k$-th item arrives. We need to show that after processing it, each of the $k$ items has probability exactly $s/k$ of being in the sample. For the $k$-th item, it's included with probability $s/k$, so $P(k\text{-th item in sample}) = s/k$. For any previous item $i$ (where $1 \leq i < k$):

\begin{align}
    P(i\text{-th item remains in sample}) &= P(i\text{-th item was in sample}) \nonumber\\
    &\quad \times P(i\text{'s not replaced}) \\
    &= \frac{s}{k-1} \times \left[1 - P(k\text{-th item is chosen}) \right. \nonumber\\
    &\quad \left. \times P(\text{item $i$ is replaced} \mid k\text{-th is chosen})\right] \\
    &= \frac{s}{k-1} \times \left[1 - \frac{s}{k} \times \frac{1}{s}\right] \\
    &= \frac{s}{k-1} \times \left[1 - \frac{1}{k}\right] \\
    &= \frac{s}{k-1} \times \frac{k-1}{k} \\
    &= \frac{s}{k}
\end{align}

This confirms that after processing the $k$-th item, each of the $k$ items has probability $s/k$ of being in the sample, maintaining uniformity.

\subsection{Implementation}

To verify these theoretical results, I implemented both algorithms in JavaScript and ran simulations to check if the sampling is uniform.

\begin{lstlisting}[language=JavaScript]
// Single-item reservoir sampling
function singleItemReservoirSampling(stream) {
    let sample = null;
    let count = 0;
    
    for (const item of stream) {
        count++;
        if (count === 1) {
            sample = item;
        } else {
            // Replace with probability 1/count
            if (Math.random() < 1/count) {
                sample = item;
            }
        }
    }
    
    return sample;
}

// Reservoir sampling of s items without replacement
function reservoirSampling(stream, s) {
    let sample = [];
    let count = 0;
    
    for (const item of stream) {
        count++;
        
        if (count <= s) {
            // Fill the reservoir with the first s items
            sample.push(item);
        } else {
            // Decide whether to include this item in the sample
            const j = Math.floor(Math.random() * count);
            if (j < s) {
                // Replace the randomly chosen item
                sample[j] = item;
            }
        }
    }
    
    return sample;
}

// Function to simulate multiple runs and track results
function runSimulations(numRuns, streamSize, sampleSize = 1) {
    const counts = {};
    
    // Initialize counts for each number
    for (let i = 1; i <= streamSize; i++) {
        counts[i] = 0;
    }
    
    for (let run = 0; run < numRuns; run++) {
        // Create a stream of numbers from 1 to streamSize
        const stream = Array.from({length: streamSize}, (_, i) => i + 1);
        
        let result;
        if (sampleSize === 1) {
            result = [singleItemReservoirSampling(stream)];
        } else {
            result = reservoirSampling(stream, sampleSize);
        }
        
        // Update counts
        result.forEach(item => {
            counts[item]++;
        });
    }
    
    // Convert to frequencies
    const frequencies = {};
    for (const [key, value] of Object.entries(counts)) {
        frequencies[key] = value / numRuns;
    }
    
    return frequencies;
}
\end{lstlisting}

\subsection{Results}

I ran 10,000 simulations with a stream of 10 items for both single-item sampling and sampling 3 items without replacement. The results confirm our theoretical analysis:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/single_item_sampling.png}
    \caption{Frequency distribution of single-item reservoir sampling over 10,000 runs. Each item has approximately 0.1 probability of being selected, as expected.}
    \label{fig:single-item-sampling}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/multi_item_sampling.png}
    \caption{Frequency distribution of 3-item reservoir sampling over 10,000 runs. Each item has approximately 0.3 probability of being selected, as expected.}
    \label{fig:multi-item-sampling}
\end{figure}

For the single-item sampling, the expected frequency is $1/10 = 0.1$ for each item. The empirical results in Figure \ref{fig:single-item-sampling} closely match this expectation, with minimal deviation from the expected value. For the 3-item sampling, the expected frequency is $3/10 = 0.3$ for each item. Again, the empirical results in Figure \ref{fig:multi-item-sampling} closely match the expectation, confirming our theoretical analysis. These simulations confirm the theoretical correctness of both algorithms, showing that they indeed maintain uniform sampling of the stream at each step.

\subsection{Why This Is Useful in Streaming Algorithms}

Reservoir sampling is particularly valuable in streaming algorithms because it maintains a representative sample without needing knowledge of the total stream size, it uses constant memory as it stores only $s$ items, it processes each item in constant time, and the sample remains uniform at each step of the process. This makes it ideal for applications like processing large data streams that don't fit in memory, online analytics where a representative sample is needed in real time, distributed systems where data is generated across multiple sources, and monitoring high-volume log data where sampling is necessary for performance.

\end{document}